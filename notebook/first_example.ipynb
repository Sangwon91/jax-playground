{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\n",
    "\n",
    "import jax\n",
    "import flax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import flax.linen as nn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dense(\n",
       "    # attributes\n",
       "    features = 5\n",
       "    use_bias = True\n",
       "    dtype = None\n",
       "    param_dtype = float32\n",
       "    precision = None\n",
       "    kernel_init = init\n",
       "    bias_init = zeros\n",
       "    dot_general = None\n",
       "    dot_general_cls = None\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Dense(features=5)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 21:34:20.862995: W external/xla/xla/service/gpu/nvptx_compiler.cc:742] The NVIDIA driver's CUDA version is 11.6 which is older than the ptxas CUDA version (11.8.89). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'params': {'bias': (5,), 'kernel': (10, 5)}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key1, key2 = random.split(random.key(0))\n",
    "x = random.normal(key1, (10,))\n",
    "params = model.init(key2, x)\n",
    "jax.tree_util.tree_map(lambda x: x.shape, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-1.3721199 ,  0.611315  ,  0.64428365,  2.2192967 , -1.1271119 ],      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(params, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{cuda(id=0)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (20, 10) ; y shape: (20, 5)\n"
     ]
    }
   ],
   "source": [
    "# Set problem dimensions.\n",
    "n_samples = 20\n",
    "x_dim = 10\n",
    "y_dim = 5\n",
    "\n",
    "# Generate random ground truth W and b.\n",
    "key = random.key(0)\n",
    "k1, k2 = random.split(key)\n",
    "W = random.normal(k1, (x_dim, y_dim))\n",
    "b = random.normal(k2, (y_dim,))\n",
    "# Store the parameters in a FrozenDict pytree.\n",
    "true_params = flax.core.freeze({'params': {'bias': b, 'kernel': W}})\n",
    "\n",
    "# Generate samples with additional noise.\n",
    "key_sample, key_noise = random.split(k1)\n",
    "x_samples = random.normal(key_sample, (n_samples, x_dim))\n",
    "y_samples = jnp.dot(x_samples, W) + b + 0.1 * random.normal(key_noise,(n_samples, y_dim))\n",
    "print('x shape:', x_samples.shape, '; y shape:', y_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as JAX version but using model.apply().\n",
    "@jax.jit\n",
    "def mse(params, x_batched, y_batched):\n",
    "  # Define the squared loss for a single pair (x,y)\n",
    "  def squared_error(x, y):\n",
    "    pred = model.apply(params, x)\n",
    "    return jnp.inner(y-pred, y-pred) / 2.0\n",
    "  # Vectorize the previous to compute the average of the loss on all samples.\n",
    "  return jnp.mean(jax.vmap(squared_error)(x_batched,y_batched), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for \"true\" W,b:  0.023639798\n",
      "Loss step 0:  35.343876\n",
      "Loss step 10:  0.5150507\n",
      "Loss step 20:  0.114045255\n",
      "Loss step 30:  0.039395206\n",
      "Loss step 40:  0.019940186\n",
      "Loss step 50:  0.014217614\n",
      "Loss step 60:  0.012428728\n",
      "Loss step 70:  0.011851474\n",
      "Loss step 80:  0.011662136\n",
      "Loss step 90:  0.0115995305\n",
      "Loss step 100:  0.011578723\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.3  # Gradient step size.\n",
    "print('Loss for \"true\" W,b: ', mse(true_params, x_samples, y_samples))\n",
    "loss_grad_fn = jax.value_and_grad(mse)\n",
    "\n",
    "@jax.jit\n",
    "def update_params(params, learning_rate, grads):\n",
    "  params = jax.tree_util.tree_map(\n",
    "      lambda p, g: p - learning_rate * g, params, grads)\n",
    "  return params\n",
    "\n",
    "for i in range(101):\n",
    "  # Perform one gradient update.\n",
    "  loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n",
    "  params = update_params(params, learning_rate, grads)\n",
    "  if i % 10 == 0:\n",
    "    print(f'Loss step {i}: ', loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "tx = optax.adam(learning_rate=learning_rate)\n",
    "opt_state = tx.init(params)\n",
    "loss_grad_fn = jax.value_and_grad(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0:  0.0115776425\n",
      "Loss step 10:  0.26137036\n",
      "Loss step 20:  0.076836\n",
      "Loss step 30:  0.03648498\n",
      "Loss step 40:  0.02203123\n",
      "Loss step 50:  0.016186504\n",
      "Loss step 60:  0.012997282\n",
      "Loss step 70:  0.012026423\n",
      "Loss step 80:  0.011765248\n",
      "Loss step 90:  0.011645812\n",
      "Loss step 100:  0.011585565\n"
     ]
    }
   ],
   "source": [
    "for i in range(101):\n",
    "  loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n",
    "  updates, opt_state = tx.update(grads, opt_state)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  if i % 10 == 0:\n",
    "    print('Loss step {}: '.format(i), loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ScaleByAdamState(count=Array(101, dtype=int32), mu={'params': {'bias': Array([8.3893799e-05, 8.6121960e-05, 8.3097213e-05, 8.4847532e-05,\n",
       "        2.8190637e-04], dtype=float32), 'kernel': Array([[ 3.9197803e-05,  3.8061680e-05,  3.5391680e-05,  3.9148064e-05,\n",
       "         -6.1675746e-06],\n",
       "        [-4.9616185e-05, -4.7960111e-05, -4.9128044e-05, -4.7233192e-05,\n",
       "          1.5249492e-05],\n",
       "        [ 1.2955531e-04,  1.3588111e-04,  1.3098563e-04,  1.3318549e-04,\n",
       "          1.0369407e-04],\n",
       "        [ 4.6785521e-05,  4.6059609e-05,  4.7439731e-05,  4.4369255e-05,\n",
       "         -1.6793149e-04],\n",
       "        [ 3.1651914e-05,  3.2143878e-05,  3.2318749e-05,  3.1883526e-05,\n",
       "          5.3564363e-05],\n",
       "        [-1.2694288e-04, -1.2165749e-04, -1.2994988e-04, -1.2423823e-04,\n",
       "         -3.2156322e-04],\n",
       "        [-1.6304413e-04, -1.6415064e-04, -1.6482500e-04, -1.6351035e-04,\n",
       "          8.4395942e-06],\n",
       "        [ 1.6221512e-04,  1.6363618e-04,  1.5648371e-04,  1.6195382e-04,\n",
       "         -9.4930510e-06],\n",
       "        [-1.5283289e-04, -1.5056762e-04, -1.5696147e-04, -1.5294740e-04,\n",
       "         -3.1967618e-04],\n",
       "        [-8.5425025e-05, -8.7753622e-05, -8.4798347e-05, -8.7161781e-05,\n",
       "          6.9359550e-05]], dtype=float32)}}, nu={'params': {'bias': Array([0.00018645, 0.00018679, 0.00018662, 0.00018669, 0.00039772],      dtype=float32), 'kernel': Array([[3.0471543e-05, 3.0329620e-05, 3.0959673e-05, 3.0372941e-05,\n",
       "         5.3685630e-04],\n",
       "        [2.7174855e-04, 2.7209721e-04, 2.7141639e-04, 2.7205708e-04,\n",
       "         5.6936849e-05],\n",
       "        [1.1330199e-04, 1.1370505e-04, 1.1338494e-04, 1.1352685e-04,\n",
       "         4.1937569e-04],\n",
       "        [1.6341743e-04, 1.6291835e-04, 1.6345635e-04, 1.6295853e-04,\n",
       "         9.2898443e-04],\n",
       "        [3.2621665e-05, 3.2917760e-05, 3.2680575e-05, 3.2832937e-05,\n",
       "         2.3842337e-04],\n",
       "        [5.2877777e-04, 5.3010118e-04, 5.2861101e-04, 5.2977755e-04,\n",
       "         6.1579223e-04],\n",
       "        [2.0862593e-04, 2.0835476e-04, 2.0867180e-04, 2.0848449e-04,\n",
       "         2.3443553e-04],\n",
       "        [2.2872169e-04, 2.2941832e-04, 2.2847932e-04, 2.2921654e-04,\n",
       "         4.9544458e-04],\n",
       "        [1.2613877e-04, 1.2631352e-04, 1.2619326e-04, 1.2641624e-04,\n",
       "         2.1811493e-04],\n",
       "        [1.3638817e-04, 1.3636556e-04, 1.3621518e-04, 1.3637530e-04,\n",
       "         1.1235375e-04]], dtype=float32)}}),\n",
       " EmptyState())"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict output\n",
      "{'params': {'bias': Array([-1.4555806, -2.0278268,  2.0790968,  1.2186172, -0.9980628],      dtype=float32), 'kernel': Array([[ 1.0098493 ,  0.18933125,  0.04458343, -0.92802316,  0.34783497],\n",
      "       [ 1.7298489 ,  0.9879658 ,  1.1640443 ,  1.1006037 , -0.10651544],\n",
      "       [-1.2029523 ,  0.2863497 ,  1.4156082 ,  0.11870176, -1.3141391 ],\n",
      "       [-1.1941417 , -0.18958248,  0.03414451,  1.3169445 ,  0.0805987 ],\n",
      "       [ 0.13851093,  1.3712997 , -1.318749  ,  0.53152126, -2.2405198 ],\n",
      "       [ 0.5629417 ,  0.8122362 ,  0.31753275,  0.53455   ,  0.90499985],\n",
      "       [-0.37926182,  1.7410471 ,  1.0790585 , -0.5039784 ,  0.9282756 ],\n",
      "       [ 0.9706411 , -1.3153212 ,  0.3368311 ,  0.80993503, -1.2018685 ],\n",
      "       [ 1.0194358 , -0.6202532 ,  1.0818852 , -1.8389667 , -0.45808962],\n",
      "       [-0.6436615 ,  0.4566898 , -1.1329143 , -0.6853882 ,  0.16831206]],      dtype=float32)}}\n",
      "Bytes output\n",
      "b'\\x81\\xa6params\\x82\\xa4bias\\xc7!\\x01\\x93\\x91\\x05\\xa7float32\\xc4\\x14wP\\xba\\xbf\\xea\\xc7\\x01\\xc0\\xec\\x0f\\x05@\\xa6\\xfb\\x9b?\\x0b\\x81\\x7f\\xbf\\xa6kernel\\xc7\\xd6\\x01\\x93\\x92\\n\\x05\\xa7float32\\xc4\\xc8\\xbeB\\x81?\\r\\xe0A>\\x1e\\x9d6=\\xed\\x92m\\xbfm\\x17\\xb2>\\xb0k\\xdd?T\\xeb|?g\\xff\\x94?\\x95\\xe0\\x8c?\\xc4$\\xda\\xbdW\\xfa\\x99\\xbfn\\x9c\\x92>\\xa62\\xb5?\\xe9\\x19\\xf3=\\xb65\\xa8\\xbf\\xa3\\xd9\\x98\\xbf\\xe9!B\\xbe\\x1d\\xdb\\x0b=\\xa3\\x91\\xa8?\\xef\\x10\\xa5=\\xcf\\xd5\\r>\\xc0\\x86\\xaf?\\xc4\\xcc\\xa8\\xbf\\xc7\\x11\\x08?\\xadd\\x0f\\xc0\\xf2\\x1c\\x10?\\xb6\\xeeO?\\xa7\\x93\\xa2>E\\xd8\\x08?\\x12\\xaeg?\\x9b.\\xc2\\xbe\\xa2\\xda\\xde?\\x97\\x1e\\x8a?\\xba\\x04\\x01\\xbfx\\xa3m?\\xef{x?r\\\\\\xa8\\xbf u\\xac>\\xe7WO?\\xd4\\xd6\\x99\\xbf\\xdf|\\x82?\\xea\\xc8\\x1e\\xbf7{\\x8a?Cc\\xeb\\xbf\\xb9\\x8a\\xea\\xbe\\x00\\xc7$\\xbf?\\xd3\\xe9>V\\x03\\x91\\xbf\\x9au/\\xbf\\xffY,>'\n"
     ]
    }
   ],
   "source": [
    "from flax import serialization\n",
    "bytes_output = serialization.to_bytes(params)\n",
    "dict_output = serialization.to_state_dict(params)\n",
    "print('Dict output')\n",
    "print(dict_output)\n",
    "print('Bytes output')\n",
    "print(bytes_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-playground-OlMwBUeJ-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
